
###########################################################################################
###                                                                                     ###
###                             Ampere A100 PCIe 40 GB Config                           ###
###                                                                                     ###
###########################################################################################


# Device Limits
-gpgpu_stack_size_limit 1024
-gpgpu_heap_size_limit 8388608
-gpgpu_kernel_launch_latency 5000
-gpgpu_thread_block_launch_latency 0
-gpgpu_max_concurrent_kernel 128

# High Level Architecture Configuration
-gpgpu_num_clusters 108
-gpgpu_num_sms_per_cluster 1
-gpgpu_num_memory_controllers 40
-gpgpu_num_sub_partition_per_memory_channel 2

# Clock Domain Frequencies in MHZ
-gpgpu_core_clock_mhz 1065.0
-gpgpu_icnt_clock_mhz 1065.0
-gpgpu_l2d_clock_mhz  1065.0
-gpgpu_dram_clock_mhz 1215.0

# SM Pipeline Config
-gpgpu_max_registers_per_sm 65536
-gpgpu_max_registers_per_cta 65536

# SM Warp Config
-gpgpu_max_threads_per_sm 2048
-gpgpu_warp_size 32
-gpgpu_max_ctas_per_sm 32

# Pipline Widths
-gpgpu_ID_OC_SP_pipeline_width 4
-gpgpu_ID_OC_DP_pipeline_width 4
-gpgpu_ID_OC_INT_pipeline_width 4
-gpgpu_ID_OC_SFU_pipeline_width 4
-gpgpu_ID_OC_MEM_pipeline_width 4
-gpgpu_OC_EX_SP_pipeline_width 4
-gpgpu_OC_EX_DP_pipeline_width 4
-gpgpu_OC_EX_INT_pipeline_width 4
-gpgpu_OC_EX_SFU_pipeline_width 4
-gpgpu_OC_EX_MEM_pipeline_width 4
-gpgpu_EX_WB_pipeline_width 8
-gpgpu_ID_OC_TENSOR_CORE_pipeline_width 4
-gpgpu_OC_EX_TENSOR_CORE_pipeline_width 4

# Number of FUs
-gpgpu_num_sp_units 4
-gpgpu_num_sfu_units 4
-gpgpu_num_dp_units 8
-gpgpu_num_int_units 8
-gpgpu_num_tensor_core_units 4
-gpgpu_num_mem_units 160

# Instruction Latencies, ADD,MAX,MUL,MAD,DIV,[SHFL]
#-gpgpu_opcode_latency_int 4,13,4,5,145,21
#-gpgpu_opcode_latency_fp 4,13,4,5,39
#-gpgpu_opcode_latency_dp 8,19,8,8,330
#-gpgpu_opcode_latency_sfu 100
#-gpgpu_opcode_latency_tensor_core 64
-gpgpu_opcode_latency_int 3,12,3,4,144,20
-gpgpu_opcode_latency_fp 3,12,3,4,38
-gpgpu_opcode_latency_dp 7,18,7,7,329
-gpgpu_opcode_latency_sfu 99
-gpgpu_opcode_latency_tensor_core 63

# Initiation Intervals, ADD,MAX,MUL,MAD,DIV,[SHFL]
-gpgpu_opcode_initiation_interval_int 2,2,2,2,8,4
-gpgpu_opcode_initiation_interval_fp 2,2,2,2,4
-gpgpu_opcode_initiation_interval_dp 4,4,4,4,130
-gpgpu_opcode_initiation_interval_sfu 8
-gpgpu_opcode_initiation_interval_tensor_core 64

# Sub Core Model, warp schedulers are isolated
-gpgpu_sub_core_model 1

# Generic Operand Collectors
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8

# Register Banks
-gpgpu_num_reg_banks 32
-gpgpu_reg_file_port_throughput 2

# Shared Memory Bankconflict Detection
-gpgpu_shmem_num_banks 64
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1
-gpgpu_coalesce_arch 80

# Warp Schedulers
-gpgpu_inst_fetch_throughput 4
-gpgpu_num_sched_per_sm 4
# for Volta, a warp scheduler can issue 1 inst per cycle
-gpgpu_max_insn_issue_per_warp 1
# for Volta, dual issue only occurs with using two different execution unit
-gpgpu_dual_issue_diff_exec_units 1

# L1/Shared Memory Configuration
# L1 cache + shared memory = 192 KB
-gpgpu_unified_l1d_size 192
-gpgpu_l1d_cache_banks 4
-gpgpu_l1d_cache_sets 4
-gpgpu_l1d_cache_block_size 128
-gpgpu_l1d_cache_associative 64
-gpgpu_l1d_latency 37
# Size of shared memory per SM (Byte)
-gpgpu_shmem_size_per_sm 167936
# Size of shared memory per CTA (Byte)
-gpgpu_shmem_size_per_cta 167936
-gpgpu_shmem_latency 37

# L2 Configuration
-gpgpu_l2d_size_per_sub_partition 512
# 32 sets, each 128 bytes 16-way for each memory sub partition (512 KB)
-gpgpu_l2d_cache_sets 256
-gpgpu_l2d_cache_block_size 128
-gpgpu_l2d_cache_associative 16
-gpgpu_dram_partition_queues_icnt_to_l2 64
-gpgpu_dram_partition_queues_l2_to_dram 64
-gpgpu_dram_partition_queues_dram_to_l2 64
-gpgpu_dram_partition_queues_l2_to_icnt 64

# Cluster Ejection Buffer
-gpgpu_num_pkts_cluster_ejection_buffer 32

# Interconnection
-gpgpu_icnt_in_buffer_limit 512
-gpgpu_icnt_out_buffer_limit 512
-gpgpu_icnt_subnets 2
-gpgpu_icnt_flit_size 40

# DRAM Configuration
-gpgpu_dram_latency 100

# Trace OpCode Latency and Initiation Interval
#-gpgpu_trace_opcode_latency_initiation_int 2,2
#-gpgpu_trace_opcode_latency_initiation_sp 2,2
#-gpgpu_trace_opcode_latency_initiation_dp 8,4
#-gpgpu_trace_opcode_latency_initiation_sfu 20,8
#-gpgpu_trace_opcode_latency_initiation_tensor 2,2
-gpgpu_trace_opcode_latency_initiation_int 2,1
-gpgpu_trace_opcode_latency_initiation_sp 2,1
-gpgpu_trace_opcode_latency_initiation_dp 8,2
-gpgpu_trace_opcode_latency_initiation_sfu 20,6
-gpgpu_trace_opcode_latency_initiation_tensor 2,1

# execute branch insts on spec unit 1
# in Volta, there is a dedicated branch unit
# <enabled>,<num_units>,<max_latency>,<ID_OC_SPEC>,<OC_EX_SPEC>,<NAME>
-gpgpu_specialized_unit_1 1,4,4,4,4,BRA
-gpgpu_trace_opcode_latency_initiation_spec_op_1 4,4

# TEX unit, make fixed latency for all tex insts
-gpgpu_specialized_unit_2 1,4,200,4,4,TEX
-gpgpu_trace_opcode_latency_initiation_spec_op_2 200,4

# tensor unit
-gpgpu_specialized_unit_3 1,4,8,4,4,TENSOR
-gpgpu_trace_opcode_latency_initiation_spec_op_3 2,2

# shared memory allocation size
-gpgpu_smem_allocation_size 256
-gpgpu_register_allocation_size 256

# L1 cache configurations
-gpgpu_l1_cache_line_size_for_reuse_distance 32
# L2 cache configurations
-gpgpu_l2_cache_line_size_for_reuse_distance 64

# dram/l1/l2 mem access latency
-gpgpu_dram_mem_access_latency 302
-gpgpu_l1_cache_access_latency 37
-gpgpu_l2_cache_access_latency 213
-gpgpu_const_mem_access_latency 8
